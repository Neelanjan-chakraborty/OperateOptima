ğŸ’¼ Project Name: Operate Optima â€“ Spark-Based ETL Optimization
Build a high-performance ETL pipeline using Apache Spark & SQL to clean and process business transaction data, with real benchmarks showing reduced processing time. The pipeline will:

Extract raw data from CSV files

Clean and transform it using PySpark

Load the cleaned output into a processed directory

Log performance metrics for optimization

ğŸ“¦ Dataset Source (Kaggle - Real Business Data)
Kaggle Dataset Name: Customer Sales Data
Direct CSV Link (once downloaded):

ruby
Copy
Edit
https://www.kaggle.com/datasets/kyanyoga/sample-sales-data/download?datasetVersionNumber=1
Or use this file from the zip:

SampleSalesData.csv

ğŸ§  Features to Implement
Extract

Load large CSVs using PySpark

Infer schema automatically

Transform

Clean null values, fix column types

Create new columns like total_revenue = quantity * price

Filter out rows with low/zero quantity or price

Load

Save cleaned data to /data/processed/ as CSV

Ensure header and overwrite enabled

Benchmark

Print ETL duration using time.time()

Log row counts before/after cleaning

ğŸ› ï¸ Folder Structure
r
Copy
Edit
operate-optima/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               <- Place downloaded Kaggle CSV here
â”‚   â””â”€â”€ processed/         <- Final cleaned data goes here
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py         <- PySpark reader
â”‚   â”œâ”€â”€ transform.py       <- Cleansing logic
â”‚   â”œâ”€â”€ load.py            <- Writer to CSV
â”‚   â””â”€â”€ config.py          <- File path variables
â”‚
â”œâ”€â”€ pipeline.py            <- Main script to run the pipeline
â”œâ”€â”€ requirements.txt       <- Dependencies
â””â”€â”€ README.md              <- Documentation
ğŸ”§ Setup Instructions
bash
Copy
Edit
# Step 1: Create virtual environment (optional)
python -m venv venv
source venv/bin/activate  # Or venv\Scripts\activate on Windows

# Step 2: Install dependencies
pip install pyspark pandas

# Step 3: Download dataset from Kaggle and unzip to `data/raw/`
# File should be located at: data/raw/SampleSalesData.csv

# Step 4: Run the pipeline
python pipeline.py
ğŸ“Œ Dataset Columns (from SampleSalesData.csv)
cs
Copy
Edit
OrderDate, Region, Rep, Item, Units, Unit Cost, Total
1/6/19, East, Jones, Pencil, 95, 1.99, 189.05
âš¡ Example Transformations (in transform.py)
Rename columns to snake_case

Convert Unit Cost and Total to float

Filter rows with Units < 10

Add profit_margin = total - (units * unit_cost)

â±ï¸ Benchmark Example (Console Output)
yaml
Copy
Edit
ğŸ”¹ Extracting...
ğŸ”¹ Transforming...
ğŸ”¹ Loading...
âœ… ETL Completed in 12.35 seconds.
Before: 5000 rows â†’ After cleaning: 4312 rows
ğŸ“ Extra Kaggle Dataset Options (for scaling later)
Sales Transactions Dataset Weekly â€“ 8000 rows

Online Retail Dataset â€“ 500K rows

Superstore Sales Data â€“ classic benchmark dataset